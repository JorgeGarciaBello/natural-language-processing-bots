{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo python3 – m download en # Este comando no me funciono,pero lo debí de correr sin sudo para ver que pasaba. # No funciona el comando. Para resolverlo fui a a la documentación oficial:  https://spacy.io/usage\n",
    "#!python -m spacy download en_core_web_sm # Comentamos esta descarga ya que el programa esta descargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'PRON')\n",
      "('am', 'AUX')\n",
      "('learning', 'VERB')\n",
      "('how', 'ADV')\n",
      "('to', 'PART')\n",
      "('build', 'VERB')\n",
      "('chatbots', 'NOUN')\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print((token.text, token.pos_)) #Prints the text and POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I am going to London next week for a meeting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'PRON')\n",
      "('am', 'AUX')\n",
      "('going', 'VERB')\n",
      "('to', 'ADP')\n",
      "('London', 'PROPN')\n",
      "('next', 'ADJ')\n",
      "('week', 'NOUN')\n",
      "('for', 'ADP')\n",
      "('a', 'DET')\n",
      "('meeting', 'NOUN')\n",
      "('.', 'PUNCT')\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print((token.text, token.pos_)) #Prints the text and POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Google release \"Move Mirror\" AI experiment that matches your pose from 80,000 images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google', 'Google', 'PROPN', 'NNP', 'compound', 'Xxxxx', True, False)\n",
      "('release', 'release', 'VERB', 'VB', 'nmod', 'xxxx', True, False)\n",
      "('\"', '\"', 'PUNCT', '``', 'punct', '\"', False, False)\n",
      "('Move', 'Move', 'PROPN', 'NNP', 'nmod', 'Xxxx', True, True)\n",
      "('Mirror', 'Mirror', 'PROPN', 'NNP', 'nmod', 'Xxxxx', True, False)\n",
      "('\"', '\"', 'PUNCT', \"''\", 'punct', '\"', False, False)\n",
      "('AI', 'AI', 'PROPN', 'NNP', 'compound', 'XX', True, False)\n",
      "('experiment', 'experiment', 'NOUN', 'NN', 'ROOT', 'xxxx', True, False)\n",
      "('that', 'that', 'DET', 'WDT', 'nsubj', 'xxxx', True, True)\n",
      "('matches', 'match', 'VERB', 'VBZ', 'relcl', 'xxxx', True, False)\n",
      "('your', '-PRON-', 'DET', 'PRP$', 'poss', 'xxxx', True, True)\n",
      "('pose', 'pose', 'NOUN', 'NN', 'dobj', 'xxxx', True, False)\n",
      "('from', 'from', 'ADP', 'IN', 'prep', 'xxxx', True, True)\n",
      "('80,000', '80,000', 'NUM', 'CD', 'nummod', 'dd,ddd', False, False)\n",
      "('images', 'image', 'NOUN', 'NNS', 'pobj', 'xxxx', True, False)\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print((token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', '-PRON-', 'PRON', 'PRP', 'nsubj', 'X', True, True)\n",
      "('am', 'be', 'AUX', 'VBP', 'aux', 'xx', True, True)\n",
      "('learning', 'learn', 'VERB', 'VBG', 'ROOT', 'xxxx', True, False)\n",
      "('how', 'how', 'ADV', 'WRB', 'advmod', 'xxx', True, True)\n",
      "('to', 'to', 'PART', 'TO', 'aux', 'xx', True, True)\n",
      "('build', 'build', 'VERB', 'VB', 'xcomp', 'xxxx', True, False)\n",
      "('chatbots', 'chatbot', 'NOUN', 'NNS', 'dobj', 'xxxx', True, False)\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print((token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "#from spacy.lookups import Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer = English.Defaults.create_lemmatizer()\n",
    "lemmatizer = nlp.Defaults.create_lemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckles']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('chuckles', 'NOUN') # 2nd param is token's part-of-speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blazing']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing','VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastest']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('fastest', 'ADJ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ugins NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intalando NLTK\n",
    "    #!pip3 install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargando NLTK para usar su STEM\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastest\n",
      "fastest\n"
     ]
    }
   ],
   "source": [
    "print(porter_stemmer.stem(\"fastest\"))\n",
    "print(snowball_stemmer.stem(\"fastest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = u\"Google has its headquarters in Mountain View, California having revenue amounted to 109.65 billion US dollars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Google', 'ORG')\n",
      "('Mountain View', 'GPE')\n",
      "('California', 'GPE')\n",
      "('109.65 billion US dollars', 'MONEY')\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print((ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "my_string = u\"Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(my_string)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mark Zuckerberg', 'PERSON')\n",
      "('May 14, 1984', 'DATE')\n",
      "('New York', 'GPE')\n",
      "('American', 'NORP')\n",
      "('Facebook', 'ORG')\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print((ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecample 3\n",
    "my_string = u\"I usually wake up at 9:00 AM. 90% of my daytime goes in learning new things.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('9:00 AM', 'TIME')\n",
      "('90%', 'PERCENT')\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print((ent.text, ent.label_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine Dragons 90% are the best band.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of context\n",
    "my_string1 = u\"Imagine Dragons 90% are the best band.\"\n",
    "my_string2 = u\"Imagine dragons come and take over the city.\"\n",
    "\n",
    "my_string1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imagine Dragons 90% are the best band."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(my_string1)\n",
    "doc2 = nlp(my_string2)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('90%', 'PERCENT')\n"
     ]
    }
   ],
   "source": [
    "for ent in doc1.ents:\n",
    "    print((ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
